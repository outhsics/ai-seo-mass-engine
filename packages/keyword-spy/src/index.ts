#!/usr/bin/env node

/**
 * Keyword Spy Module
 * è‡ªåŠ¨çˆ¬å–å¹¶åˆ†æ SEO å…³é”®è¯
 */

import { writeFileSync, mkdirSync } from 'fs';
import { join } from 'path';

interface KeywordSpyConfig {
  sources: KeywordSource[];
  outputDir: string;
  maxKeywords: number;
  minVolume: number;
  niches: string[];
}

interface KeywordSource {
  type: 'google' | 'baidu' | 'bing' | 'custom';
  url?: string;
  enabled: boolean;
}

interface KeywordData {
  keyword: string;
  volume: number;
  difficulty: number;
  cpc: number;
  trend: number;
  source: string;
  scrapedAt: Date;
}

export class KeywordSpy {
  private config: KeywordSpyConfig;
  private scraper: KeywordScraper;
  private analyzer: KeywordAnalyzer;

  constructor(config: KeywordSpyConfig) {
    this.config = config;
    this.scraper = new KeywordScraper(config);
    this.analyzer = new KeywordAnalyzer();
  }

  async execute(): Promise<KeywordData[]> {
    console.log('ğŸ” Starting keyword scraping...');

    const allKeywords: KeywordData[] = [];

    for (const niche of this.config.niches) {
      console.log(`\nğŸ“‚ Processing niche: ${niche}`);

      const nicheKeywords = await this.scrapeNiche(niche);
      const analyzedKeywords = await this.analyzer.analyze(nicheKeywords);

      allKeywords.push(...analyzedKeywords);
    }

    // æŒ‰ç…§æœç´¢é‡å’Œéš¾åº¦æ’åº
    const sortedKeywords = this.analyzer.rankByPotential(allKeywords);

    // å–å‰ N ä¸ªå…³é”®è¯
    const topKeywords = sortedKeywords.slice(0, this.config.maxKeywords);

    // ä¿å­˜ç»“æœ
    await this.saveResults(topKeywords);

    console.log(`\nâœ… Scraped ${topKeywords.length} keywords`);
    return topKeywords;
  }

  private async scrapeNiche(niche: string): Promise<KeywordData[]> {
    const keywords: KeywordData[] = [];

    for (const source of this.config.sources) {
      if (!source.enabled) continue;

      try {
        const sourceKeywords = await this.scraper.scrape(source, niche);
        keywords.push(...sourceKeywords);
      } catch (error) {
        console.error(`âŒ Failed to scrape from ${source.type}:`, error);
      }
    }

    return keywords;
  }

  private async saveResults(keywords: KeywordData[]): Promise<void> {
    const outputDir = join(process.cwd(), this.config.outputDir, 'keywords');
    mkdirSync(outputDir, { recursive: true });

    const timestamp = new Date().toISOString().split('T')[0];
    const filePath = join(outputDir, `keywords-${timestamp}.json`);

    writeFileSync(filePath, JSON.stringify(keywords, null, 2));
    console.log(`ğŸ’¾ Saved keywords to: ${filePath}`);

    // åŒæ—¶ä¿å­˜ CSV æ ¼å¼
    const csvPath = join(outputDir, `keywords-${timestamp}.csv`);
    const csvContent = this.toCSV(keywords);
    writeFileSync(csvPath, csvContent);
    console.log(`ğŸ’¾ Saved CSV to: ${csvPath}`);
  }

  private toCSV(keywords: KeywordData[]): string {
    const headers = ['Keyword', 'Volume', 'Difficulty', 'CPC', 'Trend', 'Source', 'Date'];
    const rows = keywords.map(k => [
      k.keyword,
      k.volume,
      k.difficulty,
      k.cpc,
      k.trend,
      k.source,
      k.scrapedAt.toISOString()
    ]);

    return [headers, ...rows].map(row => row.join(',')).join('\n');
  }
}

export class KeywordScraper {
  constructor(private config: KeywordSpyConfig) {}

  async scrape(source: KeywordSource, niche: string): Promise<KeywordData[]> {
    switch (source.type) {
      case 'google':
        return this.scrapeGoogleSuggestions(niche);
      case 'custom':
        return source.url ? this.scrapeCustomUrl(source.url) : [];
      default:
        return [];
    }
  }

  private async scrapeGoogleSuggestions(query: string): Promise<KeywordData[]> {
    // Google Autocomplete API
    const url = `http://suggestqueries.google.com/complete/search?client=firefox&ds=yt&q=${encodeURIComponent(query)}`;
    const response = await fetch(url);
    const data = await response.json() as [string, string[]];

    return (data[1]).map((keyword, index) => ({
      keyword,
      volume: Math.floor(Math.random() * 10000) + 100, // æ¨¡æ‹Ÿæ•°æ®
      difficulty: Math.floor(Math.random() * 100),
      cpc: Math.random() * 5,
      trend: Math.floor(Math.random() * 100),
      source: 'google-autocomplete',
      scrapedAt: new Date()
    }));
  }

  private async scrapeCustomUrl(url: string): Promise<KeywordData[]> {
    // ä½¿ç”¨ Puppeteer çˆ¬å–è‡ªå®šä¹‰ URL
    const keywords: KeywordData[] = [];
    // TODO: å®ç°å…·ä½“çš„çˆ¬å–é€»è¾‘
    return keywords;
  }
}

export class KeywordAnalyzer {
  async analyze(keywords: KeywordData[]): Promise<KeywordData[]> {
    // è®¡ç®—å…³é”®è¯æ½œåŠ›åˆ†æ•°
    return keywords.map(k => ({
      ...k,
      // æ½œåŠ› = (æœç´¢é‡ / 100) - (éš¾åº¦ * 0.5) + (è¶‹åŠ¿ * 0.3)
      trend: Math.floor((k.volume / 100) - (k.difficulty * 0.5) + (k.trend * 0.3))
    }));
  }

  rankByPotential(keywords: KeywordData[]): KeywordData[] {
    return keywords.sort((a, b) => b.trend - a.trend);
  }
}

// CLI å…¥å£
if (import.meta.url === `file://${process.argv[1]}`) {
  const config: KeywordSpyConfig = {
    sources: [
      { type: 'google', enabled: true },
      { type: 'baidu', enabled: false }
    ],
    outputDir: './data',
    maxKeywords: 1000,
    minVolume: 100,
    niches: [
      'å‰ç«¯æŠ¥é”™',
      'Reactæ•™ç¨‹',
      'TypeScriptå…¥é—¨',
      'Astroå¼€å‘',
      'SEOä¼˜åŒ–'
    ]
  };

  const spy = new KeywordSpy(config);
  spy.execute().catch(console.error);
}
